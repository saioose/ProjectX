import requests
import pandas as pd
import time
from itertools import cycle
from threading import Thread
from openpyxl import Workbook
import os

# Define your Polygon.io API Keys
api_keys = [
    '29Mti4urwzscMmlmY8yFv3H8o9kuqKPk',
    'waXCphbpzw0h1HwnpV0z4h816F3ZOMRz',
'29Mti4urwzscMmlmY8yFv3H8o9kuqKPk',
'waXCphbpzw0h1HwnpV0z4h816F3ZOMRz',
'RNjdPi4ITj4wcAsQr8iXmpbH25Dp6Dfq',
'z1FGEeEuH7pX3cBURZCJhB6K8Pi1aBDo',
'P05dX6jkwbbSjCwtFswIn4wD4IPeHqFZ',
'FQKDXHvzjP9p_mc9qrTiL569pNRqzjIh',
'kidCiIo9t41q8D_WpgOpEPCtAKwjK9sF',
'iTbWp2c5ayHe_G0UU8vn8rhrjpOwlJcx',
'A4aIdpzi9g18hikAZssUeNaAnw2fMFyU',
'1B1wpCeCS2wVGyBYut3SqUypFGhac_oD',
'CfYsDRMQ82dnMEUUglFMC1AtAVyYIzbS',
'6hzEC2es9RJKPBftdCQ16bdkrOWla5zM',
'iMVa4MAjUndkVdty_e32IpLA7BsrldGU',
'SDYwKp3rD_zdl7TGSy1m9SITGcdqtXC7',
'bwAwbsgzoK8JvdO_ndgOlYe1acSjkp8T',
'yppGGOlpObWgly19qiUQI8jB9TkWX0M_'


    # Add additional keys as needed
]

# Define the base URL for the API
base_url = 'https://api.polygon.io/v2/aggs/ticker/'

# Define the ticker symbols and other parameters
symbols = ['X:BTCUSD', 'X:ETHUSD']  # List of symbols to fetch data for
start_date = '2019-11-02'  # Start date for the data range (YYYY-MM-DD)
end_date = '2025-01-08'  # End date for the data range (YYYY-MM-DD)
multiplier = 15  # Time span multiplier (e.g., 5 = 5 minutes)
timespan = 'minute'  # Time span for aggregation ('minute', 'hour', etc.)

# Set pandas display options to show all rows and columns
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1000)
pd.set_option('display.max_colwidth', None)

def write_to_csv_incrementally(filename, data):
    """Write data to CSV incrementally."""
    df = pd.DataFrame(data)
    if not df.empty:
        df.rename(columns={'c': 'close', 'o': 'open', 'h': 'high', 'l': 'low', 'v': 'volume', 't': 'timestamp'}, inplace=True)
        write_header = not os.path.exists(filename)
        df.to_csv(filename, mode='a', header=write_header, index=False)

def write_to_excel(filename, data, sheet_name_prefix="Sheet"):
    """Write data to Excel, splitting across sheets if necessary."""
    df = pd.DataFrame(data)
    if not df.empty:
        df.rename(columns={'c': 'close', 'o': 'open', 'h': 'high', 'l': 'low', 'v': 'volume', 't': 'timestamp'}, inplace=True)

        with pd.ExcelWriter(filename, engine='openpyxl') as writer:
            max_rows_per_sheet = 1048576
            num_sheets = (len(df) // max_rows_per_sheet) + 1

            for i in range(num_sheets):
                start_row = i * max_rows_per_sheet
                end_row = start_row + max_rows_per_sheet
                sheet_data = df.iloc[start_row:end_row]
                sheet_name = f"{sheet_name_prefix}_{i + 1}"
                sheet_data.to_excel(writer, sheet_name=sheet_name, index=False)

def fetch_crypto_data_efficiently(crypto_pair, start_date, end_date, multiplier, timespan, api_keys):
    print(f"Starting data fetch for {crypto_pair} from {start_date} to {end_date}")

    url = f'{base_url}{crypto_pair}/range/{multiplier}/{timespan}/{start_date}/{end_date}'
    api_key_cycle = cycle(api_keys)
    all_results = []
    expected_interval = multiplier * 60 * 1000  # Expected interval in milliseconds

    # Track requests made with each API key to avoid exceeding limits
    requests_made = {key: 0 for key in api_keys}
    time_window_start = time.time()

    csv_filename = f'crypto_data_{crypto_pair.replace(":", "_")}.csv'
    excel_filename = f'crypto_data_{crypto_pair.replace(":", "_")}.xlsx'

    while url:
        api_key = next(api_key_cycle)
        print(f"Using API key: {api_key}")

        # Ensure we respect the 5 requests per minute limit for each key
        if requests_made[api_key] >= 5:
            elapsed = time.time() - time_window_start
            if elapsed < 60:
                wait_time = 60 - elapsed
                print(f"Rate limit reached for API key {api_key}. Waiting {wait_time:.2f} seconds...")
                time.sleep(wait_time)
                time_window_start = time.time()
                requests_made = {key: 0 for key in api_keys}  # Reset counts after the window

        params = {'adjusted': 'true', 'sort': 'asc', 'apiKey': api_key}
        print(f"Sending request to URL: {url} with params: {params}")
        response = requests.get(url, params=params)

        if response.status_code == 200:
            print("Request successful. Processing data...")
            data = response.json()
            if 'results' in data and data['results']:
                write_to_csv_incrementally(csv_filename, data['results'])
                all_results.extend(data['results'])
            url = data.get('next_url', None)
            print("Moving to next URL..." if url else "No more pages to fetch.")
        elif response.status_code == 429:
            print(f"Rate limit hit for API key: {api_key}. Retrying in 15 seconds...")
            time.sleep(15)  # Backoff before retrying
        else:
            print(f"Failed request with status code {response.status_code}: {response.text}")
            break

        requests_made[api_key] += 1
        print(f"Requests made with {api_key}: {requests_made[api_key]}")

    # Sort all results in chronological order by timestamp
    sorted_results = pd.DataFrame(all_results).sort_values(by='timestamp').reset_index(drop=True)
    write_to_excel(excel_filename, sorted_results)
    print(f"Data exported successfully to {csv_filename} and {excel_filename}.")

def fetch_data_for_all_symbols():
    threads = []
    for symbol in symbols:
        thread = Thread(target=fetch_crypto_data_efficiently, args=(symbol, start_date, end_date, multiplier, timespan, api_keys))
        threads.append(thread)
        thread.start()

    for thread in threads:
        thread.join()

# Main execution
if __name__ == "__main__":
    fetch_data_for_all_symbols()
